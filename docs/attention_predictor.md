### Attention Predictor

A next-token prediction architecture featuring embedded input and output based on the attention mechanism.

**Architecture**

(Remember diagrams in AI are read from the bottom to the top for some unknown reason).

![attention predictor architecture](./attention_predictor.png)

**Description**

This model could be described as a simplistic decoder-only transformer.

**Features**

 - Multi-Head Attention.
 - Embedded output instead of one-hot dictionnary output.

**Work notes**

Things that helped while building this model:
- Adding noise to the input.
- Repeating and shuffling lines of the corpus.
- Logging min / max values of each layer output to debug NaNs propagation.
- Clipping some problematic layer outputs using `clamp` and later `tanh` (more performant and derivable) is a solution that helped getting rid of NaNs caused by too big outputs.

**Training corpus**

The model is trained on a corpus generated by a mix of LLMs, translation tools, manual input by me.

**Example output**


```
The birds|> are flying. The cat|> is sleeping on the rug. The dog|> is running after the ball.
The fish|> is swimming in the sea. A sailboat|> is sailing in the sea. A carrot|> is growing in the garden.
```

Some issues so far:

 - I have no concept of a stop token so far, that's why there is some of the next sentence at the end of these examples.

**Conclusion**
(section left empty because I'm still working on this model)

**What's next?**
(section left empty because I'm still working on this model)
